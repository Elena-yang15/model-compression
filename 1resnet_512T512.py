# -*- coding: utf-8 -*-
"""computecanada_sample.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nacw9G1OkeB5zGWGrhRFnXw7B4HNyDlo
"""

# -*- coding: utf-8 -*-
"""embedding_D50_64_tra10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hoA54ghaR-41JuzWn6MP0znw12QkS2MS
"""

import torch
import torchvision
import numpy as np
import random
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim


n_epochs = 50
batch_size_train = 50
batch_size_hard_train = 2000
batch_size_test = 500
learning_rate = 0.1
momentum = 0.9
log_interval = 10

random_seed = 123

np.random.seed(random_seed)
random.seed(random_seed)

torch.manual_seed(random_seed)

torch.backends.cudnn.enabled = False
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True

####################################################
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Assuming that we are on a CUDA machine, this should print a CUDA device:

print(device)
##############################################################

norm_mean =  [x / 255 for x in [125.3, 123.0, 113.9]]
norm_std =  [x / 255 for x in [63.0, 62.1, 66.7]]

transform_train = torchvision.transforms.Compose([
                                                  torchvision.transforms.RandomCrop(32, padding=4, fill=128),
                                                  torchvision.transforms.RandomHorizontalFlip(),
                                                  #torchvision.transforms.RandomCrop(32, padding=2),
                                                  torchvision.transforms.ToTensor(),
                                                  torchvision.transforms.Normalize(norm_mean, norm_std)
                                                  
                                                 ])

transform_test = torchvision.transforms.Compose([
                                                  
                                                  torchvision.transforms.ToTensor(),
                                                  torchvision.transforms.Normalize(norm_mean, norm_std)
                                                 ])


###################################################################
from torchvision import datasets, transforms
training_dataset = datasets.CIFAR10('./dataset/cifar10/', download=False, train=True, transform= transform_train)

train_loader = torch.utils.data.DataLoader(training_dataset,   ###注意和mnist区别
  batch_size=batch_size_train, shuffle=True, num_workers=0, pin_memory=True)

hard_train_loader = torch.utils.data.DataLoader(training_dataset,   ###注意和mnist区别
  batch_size=batch_size_hard_train, shuffle=True, num_workers=0, pin_memory=True)


test_dataset = datasets.CIFAR10('./dataset/cifar10/', download=False, train=False, transform= transform_test)

test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size_test, shuffle=True, num_workers=0, pin_memory=True)

######################################################################3


class AttentionCNNLayer(torch.nn.Module):
    def __init__(self, embed_dim, channels, kernel_size, bias, stride):
        super().__init__()

        self.in_channels = channels[0]  # CNN
        self.out_channels = channels[1]  # CNN
        self.d = embed_dim  # the dimension of embedding vectors
        self.num_kernels = channels[0] * channels[1]  # the number of weight clusters
        self.kernel_size = kernel_size
        self.n = kernel_size * kernel_size  # 2D kernel是 a x a维度的，n = a x a, 9
        self.Q = torch.nn.Parameter(
            torch.randn([self.out_channels, self.in_channels, self.d]))  # embedding vectors, trainable, random, 64,3,32
        torch.nn.init.normal_(self.Q, mean=0, std=0.5)

        self.in_channels = channels[0]
        self.out_channels = channels[1]
        if bias:
            self.bias = torch.nn.Parameter(torch.randn(self.out_channels))
        else:
            self.bias = None

        self.stride = stride

    def forward(self, x, K, V):
        p = self.get_p(K, 1)
        output = self.attention(p, V)
        # kernel_weights = output.T.view((self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))
        # kernel_weights_test = output_test.T.view((self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))
        x = torch.nn.functional.conv2d(x, output, bias=self.bias, stride=self.stride, padding=(1, 1))
        # x_test = torch.nn.functional.conv2d(x, kernel_weights_test, bias = self.bias, stride = (1,1),padding = (1,1))
        loss2 = self.get_loss2(p)
        return x, loss2

    def get_p(self, K, m):
        p_scores = torch.einsum('ijk, km -> ijm', self.Q, K)  # 64,3,32 x 32,16----64,3,16
        p_scores = m * p_scores
        p = torch.nn.functional.softmax(p_scores, dim=2)  # Matrix p: k x m, 10 x 512, 每一列对应一个kernel的p分布情况

        return p

    def hard_weighting(self, x, K, V, m):
        p = self.get_p(K, m)
        output = self.attention(p, V)
        # kernel_weights = output.T.view((self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))
        # kernel_weights_test = output_test.T.view((self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))
        x = torch.nn.functional.conv2d(x, output, bias=self.bias, stride=self.stride, padding=(1, 1))
        # x_test = torch.nn.functional.conv2d(x, kernel_weights_test, bias = self.bias, stride = (1,1),padding = (1,1))
        loss2 = self.get_loss2(p)
        return x, loss2

    def attention(self, p, V):

        output_weights = torch.einsum('ijk, kmn -> ijmn', p, V)  # 64,3,16 x 16,3,3 ---- 64,3,3,3

        return output_weights

    def get_loss2(self, p):

        a = torch.max(p, dim=2)
        return torch.sum(torch.square(a.values - 1.))

    def get_loss_q(self):

        loss_q = torch.sum(self.Q.pow(2))

        return loss_q


####################################################################
class AttentionTranLayer(torch.nn.Module):
    def __init__(self, embed_dim, in_channel, out_channel, kernel_size, bias):
        super().__init__()

        self.in_channels = in_channel  # CNN
        self.out_channels = out_channel  # CNN
        # self.incluster_size = cluster_size[0]
        # self.outcluster_size = cluster_size[1]

        self.d = embed_dim  # the dimension of embedding vectors
        self.num_kernels = in_channel * out_channel # the number of weight clusters
        self.kernel_size = kernel_size
        self.n = kernel_size * kernel_size  # 2D kernel是 a x a维度的，n = a x a, 9
        self.Q = torch.nn.Parameter(
            torch.randn(self.out_channels, self.in_channels, self.d))  # embedding vectors, trainable, random, 64,3,32
        torch.nn.init.normal_(self.Q, mean=0, std=0.5)

        if bias:
            self.bias = torch.nn.Parameter(torch.randn(self.out_channels))
        else:
            self.bias = None

    def forward(self, x, K, V):
        p = self.get_p(K, 1)
        output = self.attention(p, V)
        # kernel_weights = output.T.view((self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))
        # kernel_weights_test = output_test.T.view((self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))
        x = torch.nn.functional.conv2d(x, output, bias=self.bias, stride=(2, 2), padding=0)
        # x_test = torch.nn.functional.conv2d(x, kernel_weights_test, bias = self.bias, stride = (1,1),padding = (1,1))
        loss2 = self.get_loss2(p)
        return x, loss2

    def get_p(self, K, m):
        p_scores = torch.einsum('ijk, km -> ijm', self.Q, K)  # 64,3,32 x 32,16----64,3,16
        p_scores = m * p_scores
        p = torch.nn.functional.softmax(p_scores, dim=2)  # Matrix p: k x m, 10 x 512, 每一列对应一个kernel的p分布情况

        return p

    def hard_weighting(self, x, K, V, m):
        p = self.get_p(K, m)
        output = self.attention(p, V)
        # kernel_weights = output.T.view((self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))
        # kernel_weights_test = output_test.T.view((self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))
        x = torch.nn.functional.conv2d(x, output, bias=self.bias, stride=(2, 2), padding=0)
        # x_test = torch.nn.functional.conv2d(x, kernel_weights_test, bias = self.bias, stride = (1,1),padding = (1,1))
        loss2 = self.get_loss2(p)
        return x, loss2

    def attention(self, p, V):

        output_weights = torch.einsum('ijk, kmn -> ijmn', p, V)  # 64,3,16 x 16,3,3 ---- 64,3,3,3
        #kernel_weights = output_weights.view((self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))

        return output_weights

    def get_loss2(self, p):

        a = torch.max(p, dim=2)
        return torch.sum(torch.square(a.values - 1.))

    def get_loss_q(self):

        loss_q = torch.sum(self.Q.pow(2))

        return loss_q


################################################################################


R = [64, 64, 64, 128, 128, 256, 256, 512]
M = [64, 64, 128, 128, 256, 256, 512, 512]


class ClusterCNN(nn.Module):
    def __init__(self, num_classes, dropout, embe_dim, num_clusters, kernel_size, embe_dim_T,
                 num_clusters_T, kernel_size_T):
        super(ClusterCNN, self).__init__()
        self.d = embe_dim  # the dimension of embedding vectors
        self.k = num_clusters  # the number of weight clusters
        self.n = kernel_size  # kernel_size
        # self.K = torch.randn(self.d, self.k)  # fixed embedding matrix, 32 x 16
        self.V = torch.nn.Parameter(torch.randn([self.k, self.n, self.n]))  # trainable shared weights, 16 x 3 x 3
        # print(self.K)

        self.register_buffer('K', torch.randn([self.d, self.k]))
        torch.nn.init.normal_(self.K, mean=0, std=1.5)
        ###########################################################

        ############ transition layer ##################
        self.Vt = torch.nn.Parameter(torch.randn(num_clusters_T, kernel_size_T,
                                                 kernel_size_T))  # trainable shared weights, 16 x 3 x 3
        # print(self.K)

        self.register_buffer('Kt', torch.randn(embe_dim_T, num_clusters_T))
        torch.nn.init.normal_(self.Kt, mean=0, std=1.5)

        ############################################################
        self.blocks = self.make_block_layers(R)
        self.residuals = self.make_residual_layers(M, dropout)

        ###########################################
        self.atten_cnn0 = AttentionCNNLayer(embe_dim, [3, 64], kernel_size, bias=False, stride=(1, 1))
        self.block1 = self.blocks[0]
        self.atten_cnn1_0 = AttentionCNNLayer(embe_dim, [64, 64], kernel_size, bias=True, stride=(1, 1))
        self.residual1 = self.residuals[0]
        self.atten_cnn1_1 = AttentionCNNLayer(embe_dim, [64, 64], kernel_size, bias=True, stride=(1, 1))  #####
        self.block2 = self.blocks[1]
        self.atten_cnn2_0 = AttentionCNNLayer(embe_dim, [64, 64], kernel_size, bias=True, stride=(1, 1))
        self.residual2 = self.residuals[1]
        self.atten_cnn2_1 = AttentionCNNLayer(embe_dim, [64, 64], kernel_size, bias=True, stride=(1, 1))  #####
        self.block3 = self.blocks[2]
        self.atten_cnn3_0 = AttentionCNNLayer(embe_dim, [64, 128], kernel_size, bias=True, stride=(2, 2))
        self.residual3 = self.residuals[2]
        self.atten_cnn3_1 = AttentionCNNLayer(embe_dim, [128, 128], kernel_size, bias=True, stride=(1, 1))  #####short1
        self.block4 = self.blocks[3]
        self.atten_cnn4_0 = AttentionCNNLayer(embe_dim, [128, 128], kernel_size, bias=True, stride=(1, 1))
        self.residual4 = self.residuals[3]
        self.atten_cnn4_1 = AttentionCNNLayer(embe_dim, [128, 128], kernel_size, bias=True, stride=(1, 1))  ######
        self.block5 = self.blocks[4]
        self.atten_cnn5_0 = AttentionCNNLayer(embe_dim, [128, 256], kernel_size, bias=True, stride=(2, 2))
        self.residual5 = self.residuals[4]
        self.atten_cnn5_1 = AttentionCNNLayer(embe_dim, [256, 256], kernel_size, bias=True, stride=(1, 1))  ####short2
        self.block6 = self.blocks[5]
        self.atten_cnn6_0 = AttentionCNNLayer(embe_dim, [256, 256], kernel_size, bias=True, stride=(1, 1))
        self.residual6 = self.residuals[5]
        self.atten_cnn6_1 = AttentionCNNLayer(embe_dim, [256, 256], kernel_size, bias=True, stride=(1, 1))  ####

        self.block7 = self.blocks[6]
        self.atten_cnn7_0 = AttentionCNNLayer(embe_dim, [256, 512], kernel_size, bias=True, stride=(2, 2))
        self.residual7 = self.residuals[6]
        self.atten_cnn7_1 = AttentionCNNLayer(embe_dim, [512, 512], kernel_size, bias=True, stride=(1, 1))  ####short3
        self.block8 = self.blocks[7]
        self.atten_cnn8_0 = AttentionCNNLayer(embe_dim, [512, 512], kernel_size, bias=True, stride=(1, 1))
        self.residual8 = self.residuals[7]
        self.atten_cnn8_1 = AttentionCNNLayer(embe_dim, [512, 512], kernel_size, bias=True, stride=(1, 1))  #####

        ########################################################################
        # self.shortcut1 = nn.Conv2d(64, 128, kernel_size=1, stride=(2,2), padding=0)
        self.atten_shortcut1 = AttentionTranLayer(embe_dim_T,  64, 128, kernel_size_T, bias=True)
        # self.shortcut2 = nn.Conv2d(128, 256, kernel_size=1, stride=(2,2), padding=0)
        self.atten_shortcut2 = AttentionTranLayer(embe_dim_T,  128, 256, kernel_size_T, bias=True)
        # self.shortcut3 = nn.Conv2d(256, 512, kernel_size=1, stride=(2,2), padding=0)
        self.atten_shortcut3 = AttentionTranLayer(embe_dim_T,  256, 512, kernel_size_T, bias=True)

        ############################################################################
        self.classifier = nn.Sequential(
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.AvgPool2d(4),
            nn.Flatten(),
            nn.Dropout(dropout),
            nn.Linear(512, num_classes)
        )

    def make_block_layers(self, R):
        layers_blocks = []
        for r in R:
            layer_block = []
            layer_block.append(nn.BatchNorm2d(r))
            layer_block.append(nn.ReLU(inplace=True))
            layers_blocks.append(nn.Sequential(*layer_block))

        return layers_blocks

    def make_residual_layers(self, M, dropout):
        layers_residuals = []
        for m in M:
            layer_residual = []
            layer_residual.append(nn.BatchNorm2d(m))
            layer_residual.append(nn.ReLU(inplace=True))
            layer_residual.append(nn.Dropout(dropout))
            layers_residuals.append(nn.Sequential(*layer_residual))

        return layers_residuals

    def get_k(self):
        return self.K

    def forward(self, x):
        loss_p = 0
        loss_t = 0

        x, loss2 = self.atten_cnn0(x, self.K, self.V)
        loss_p += loss2
        #####block1#####
        x = self.block1(x)
        x1, loss2 = self.atten_cnn1_0(x, self.K, self.V)
        loss_p += loss2
        x1 = self.residual1(x1)
        x1, loss2 = self.atten_cnn1_1(x1, self.K, self.V)
        loss_p += loss2
        x = F.relu(x + x1)
        #####block2#####
        x = self.block2(x)
        x1, loss2 = self.atten_cnn2_0(x, self.K, self.V)
        loss_p += loss2
        x1 = self.residual2(x1)
        x1, loss2 = self.atten_cnn2_1(x1, self.K, self.V)
        loss_p += loss2
        x = F.relu(x + x1)
        #####block3#####
        x = self.block3(x)
        x1, loss2 = self.atten_cnn3_0(x, self.K, self.V)
        loss_p += loss2
        x1 = self.residual3(x1)
        x1, loss2 = self.atten_cnn3_1(x1, self.K, self.V)
        loss_p += loss2
        x2, loss = self.atten_shortcut1(x, self.Kt, self.Vt)
        loss_t += loss
        x = F.relu(x2 + x1)
        #####block4#####
        x = self.block4(x)
        x1, loss2 = self.atten_cnn4_0(x, self.K, self.V)
        loss_p += loss2
        x1 = self.residual4(x1)
        x1, loss2 = self.atten_cnn4_1(x1, self.K, self.V)
        loss_p += loss2
        x = F.relu(x + x1)
        #####block5#####
        x = self.block5(x)
        x1, loss2 = self.atten_cnn5_0(x, self.K, self.V)
        loss_p += loss2
        x1 = self.residual5(x1)
        x1, loss2 = self.atten_cnn5_1(x1, self.K, self.V)
        loss_p += loss2
        # x2 = self.shortcut2(x)
        x2, loss = self.atten_shortcut2(x, self.Kt, self.Vt)
        loss_t += loss
        x = F.relu(x2 + x1)
        #####block6#####
        x = self.block6(x)
        x1, loss2 = self.atten_cnn6_0(x, self.K, self.V)
        loss_p += loss2
        x1 = self.residual6(x1)
        x1, loss2 = self.atten_cnn6_1(x1, self.K, self.V)
        loss_p += loss2
        x = F.relu(x + x1)
        #####block7#####
        x = self.block7(x)
        x1, loss2 = self.atten_cnn7_0(x, self.K, self.V)
        loss_p += loss2
        x1 = self.residual7(x1)
        x1, loss2 = self.atten_cnn7_1(x1, self.K, self.V)
        loss_p += loss2
        # x2 = self.shortcut3(x)
        x2, loss = self.atten_shortcut3(x, self.Kt, self.Vt)
        loss_t += loss
        x = F.relu(x2 + x1)
        #####block8#####
        x = self.block8(x)
        x1, loss2 = self.atten_cnn8_0(x, self.K, self.V)
        loss_p += loss2
        x1 = self.residual8(x1)
        x1, loss2 = self.atten_cnn8_1(x1, self.K, self.V)
        loss_p += loss2
        x = F.relu(x + x1)

        out = self.classifier(x)
        return out, loss_p, loss_t

    ##################### only for testing ##################
    def hard_test(self, x):
        loss_p = 0
        loss_t = 0

        x, loss2 = self.atten_cnn0.hard_weighting(x, self.K, self.V, m=1e30)
        loss_p += loss2
        #####block1#####
        x = self.block1(x)
        x1, loss2 = self.atten_cnn1_0.hard_weighting(x, self.K, self.V, m=1e30)
        loss_p += loss2
        x1 = self.residual1(x1)
        x1, loss2 = self.atten_cnn1_1.hard_weighting(x1, self.K, self.V, m=1e30)
        loss_p += loss2
        x = F.relu(x + x1)
        #####block2#####
        x = self.block2(x)
        x1, loss2 = self.atten_cnn2_0.hard_weighting(x, self.K, self.V, m=1e30)
        loss_p += loss2
        x1 = self.residual2(x1)
        x1, loss2 = self.atten_cnn2_1.hard_weighting(x1, self.K, self.V, m=1e30)
        loss_p += loss2
        x = F.relu(x + x1)
        #####block3#####
        x = self.block3(x)
        x1, loss2 = self.atten_cnn3_0.hard_weighting(x, self.K, self.V, m=1e30)
        loss_p += loss2
        x1 = self.residual3(x1)
        x1, loss2 = self.atten_cnn3_1.hard_weighting(x1, self.K, self.V, m=1e30)
        loss_p += loss2
        # x2 = self.shortcut1(x)
        x2, loss = self.atten_shortcut1.hard_weighting(x, self.Kt, self.Vt, m=1e30)
        loss_t += loss
        x = F.relu(x2 + x1)
        #####block4#####
        x = self.block4(x)
        x1, loss2 = self.atten_cnn4_0.hard_weighting(x, self.K, self.V, m=1e30)
        loss_p += loss2
        x1 = self.residual4(x1)
        x1, loss2 = self.atten_cnn4_1.hard_weighting(x1, self.K, self.V, m=1e30)
        loss_p += loss2
        x = F.relu(x + x1)
        #####block5#####
        x = self.block5(x)
        x1, loss2 = self.atten_cnn5_0.hard_weighting(x, self.K, self.V, m=1e30)
        loss_p += loss2
        x1 = self.residual5(x1)
        x1, loss2 = self.atten_cnn5_1.hard_weighting(x1, self.K, self.V, m=1e30)
        loss_p += loss2
        # x2 = self.shortcut2(x)
        x2, loss = self.atten_shortcut2.hard_weighting(x, self.Kt, self.Vt, m=1e30)
        loss_t += loss
        x = F.relu(x2 + x1)
        #####block6#####
        x = self.block6(x)
        x1, loss2 = self.atten_cnn6_0.hard_weighting(x, self.K, self.V, m=1e30)
        loss_p += loss2
        x1 = self.residual6(x1)
        x1, loss2 = self.atten_cnn6_1.hard_weighting(x1, self.K, self.V, m=1e30)
        loss_p += loss2
        x = F.relu(x + x1)
        #####block7#####
        x = self.block7(x)
        x1, loss2 = self.atten_cnn7_0.hard_weighting(x, self.K, self.V, m=1e30)
        loss_p += loss2
        x1 = self.residual7(x1)
        x1, loss2 = self.atten_cnn7_1.hard_weighting(x1, self.K, self.V, m=1e30)
        loss_p += loss2
        # x2 = self.shortcut3(x)
        x2, loss = self.atten_shortcut3.hard_weighting(x, self.Kt, self.Vt, m=1e30)
        loss_t += loss
        x = F.relu(x2 + x1)
        #####block8#####
        x = self.block8(x)
        x1, loss2 = self.atten_cnn8_0.hard_weighting(x, self.K, self.V, m=1e30)
        loss_p += loss2
        x1 = self.residual8(x1)
        x1, loss2 = self.atten_cnn8_1.hard_weighting(x1, self.K, self.V, m=1e30)
        loss_p += loss2
        x = F.relu(x + x1)

        out = self.classifier(x)
        return out, loss_p, loss_t

    def loss_q(self):
        lossq = 0
        lossq += self.atten_cnn0.get_loss_q()
        lossq += self.atten_cnn1_0.get_loss_q()
        lossq += self.atten_cnn1_1.get_loss_q()
        lossq += self.atten_cnn2_0.get_loss_q()
        lossq += self.atten_cnn2_1.get_loss_q()
        lossq += self.atten_cnn3_0.get_loss_q()
        lossq += self.atten_cnn3_1.get_loss_q()
        lossq += self.atten_cnn4_0.get_loss_q()
        lossq += self.atten_cnn4_1.get_loss_q()
        lossq += self.atten_cnn5_0.get_loss_q()
        lossq += self.atten_cnn5_1.get_loss_q()
        lossq += self.atten_cnn6_0.get_loss_q()
        lossq += self.atten_cnn6_1.get_loss_q()
        lossq += self.atten_cnn7_0.get_loss_q()
        lossq += self.atten_cnn7_1.get_loss_q()
        lossq += self.atten_cnn8_0.get_loss_q()
        lossq += self.atten_cnn8_1.get_loss_q()
        lossq = lossq / 2

        return lossq


#####################################################################


##################################################################################
def clusternet(num_classes, dropout):
    return ClusterCNN(num_classes, dropout, embe_dim=55, num_clusters=512, kernel_size=3, embe_dim_T=55,
                       num_clusters_T=512, kernel_size_T=1)


network = clusternet(num_classes=10, dropout=0.1)
print(network)

network.to(device)
####################################################################################

##################################################################
class MyLoss(nn.Module):
    def __init__(self, total_kernels, total_kernels_t):
        super().__init__()
        # self.trade_off1 = trade_off1
        # self.trade_off2 = trade_off2
        self.total_kernels = total_kernels
        self.total_kernels_t = total_kernels_t
        # self.rate = increase_rate

    def forward(self, loss_p, loss_t, predicts, target, trade_off_p, trade_off_t):
        loss1 = torch.nn.functional.cross_entropy(predicts, target)

        loss2 = 1. / self.total_kernels * loss_p
        loss3 = 1. / self.total_kernels_t * loss_t

        loss = loss1 + trade_off_p * loss2 + trade_off_t * loss3

        return loss, loss1, loss2, loss3

#########################################################################
my_loss = MyLoss(total_kernels = 1220800, total_kernels_t = 172032)
my_loss.to(device)

#########################################################################
def train(epoch, network, optimizer, trade_off_p, trade_off_t):
    network.train()
    ave_loss = []
    ave_loss2 = []
    ave_loss1 = []
    ave_loss3 = []
    correct = 0

    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)

        optimizer.zero_grad()
        output, loss_train, loss_t = network(data)
        # lossv = torch.sum(network.V.pow(2)) / 2
        # lossq = network.loss_q()

        loss, loss1, loss2, loss3 = my_loss(loss_train, loss_t, output, target, trade_off_p,
                                            trade_off_t)  # trade off = 0.002

        loss.backward()
        optimizer.step()
        ave_loss.append(loss.item())
        ave_loss1.append(loss1.item())
        ave_loss2.append(loss2.item())
        ave_loss3.append(loss3.item())

        pred = output.data.max(1, keepdim=True)[1]
        correct += pred.eq(target.data.view_as(pred)).sum()

        if batch_idx % log_interval == 0:
            print(
                'Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}, \tLoss1: {:.6f}, \tLossp: {:.6f}, \tLosst: {:.6f}'.format(
                    epoch, batch_idx * len(data), len(train_loader.dataset),
                           100. * batch_idx / len(train_loader), loss.item(), loss1.item(), loss2.item(), loss3.item()))

    ave_all = sum(ave_loss) / len(ave_loss)
    ave1 = sum(ave_loss1) / len(ave_loss1)
    ave2 = sum(ave_loss2) / len(ave_loss2)
    ave3 = sum(ave_loss3) / len(ave_loss3)

    print("Soft training: Average loss:{},\tAverage loss1:{},\tAverage loss2:{},\tAverage losst:{}".format(
        ave_all, ave1, ave2, ave3
    ))
    accuracy = 100. * correct / len(train_loader.dataset)
    print("epochs: {},Soft training Accuracy: {}/{} ({:.0f}%)".format(
        epoch, correct, len(train_loader.dataset),
        100. * correct / len(train_loader.dataset)))

    return ave_all, ave1, ave2, ave3, accuracy

###############################################################


def soft_test(network, trade_off_p, trade_off_t):
    network.eval()
    test_loss = 0
    correct = 0
    test_loss1 = 0
    test_loss2 = 0
    test_loss3 = 0

    print('Start testing:')
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)

            output, loss_test, loss_t_test = network(data)

            test_loss_all, test_loss_1, test_loss_2, test_loss_3 = my_loss(loss_test, loss_t_test, output, target,
                                                                           trade_off_p, trade_off_t)  #
            test_loss += test_loss_all.item()
            test_loss1 += test_loss_1.item()
            test_loss2 += test_loss_2.item()
            test_loss3 += test_loss_3.item()

            pred = output.data.max(1, keepdim=True)[1]
            correct += pred.eq(target.data.view_as(pred)).sum()

    test_loss /= len(test_loader.dataset) / batch_size_test
    test_loss1 /= len(test_loader.dataset) / batch_size_test
    test_loss2 /= len(test_loader.dataset) / batch_size_test
    test_loss3 /= len(test_loader.dataset) / batch_size_test

    print(
        '\nTest set: Soft: loss: {:.4f},loss1: {:.4f},loss2: {:.4f}, losst: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
            test_loss, test_loss1, test_loss2, test_loss3, correct, len(test_loader.dataset),
            100. * correct / len(test_loader.dataset)))

    return test_loss, test_loss1, test_loss2, test_loss3, 100. * correct / len(test_loader.dataset)


##################################################################################


def hard_training_accu(network):
    network.eval()
    test_loss = 0
    correct = 0
    # test_loss1 = 0
    test_loss2 = 0
    print('Hard training: Start testing:')
    with torch.no_grad():
        for data, target in hard_train_loader:
            data, target = data.to(device), target.to(device)
            output, _, _ = network.hard_test(data)
            test_loss_all = torch.nn.functional.cross_entropy(output, target)
            test_loss += test_loss_all.item()
            # test_loss1 += test_loss_1.item()
            # test_loss2 += loss_2.item()

            pred = output.data.max(1, keepdim=True)[1]
            correct += pred.eq(target.data.view_as(pred)).sum()

    test_loss /= len(hard_train_loader.dataset) / batch_size_hard_train
    # test_loss1 /= len(hard_train_loader.dataset) / batch_size_hard_train
    # test_loss2 /= len(hard_train_loader.dataset) / batch_size_hard_train
    print('\nTraining set: Hard: loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(hard_train_loader.dataset),
        100. * correct / len(hard_train_loader.dataset)))

    return test_loss, 100. * correct / len(hard_train_loader.dataset)


#########################################################################


def hard_testing_accu(network):
    network.eval()
    test_loss = 0
    correct = 0
    # test_loss1 = 0
    test_loss2 = 0
    test_loss3 = 0
    print('Hard testing: Start testing:')
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output, loss2, loss3 = network.hard_test(data)
            test_loss_all = torch.nn.functional.cross_entropy(output, target)
            test_loss += test_loss_all.item()
            # test_loss1 += test_loss_1.item()
            test_loss2 += loss2.item()
            test_loss3 += loss3.item()

            pred = output.data.max(1, keepdim=True)[1]
            correct += pred.eq(target.data.view_as(pred)).sum()

    test_loss /= len(test_loader.dataset) / batch_size_test
    # test_loss1 /= len(test_loader.dataset) / batch_size_test
    test_loss2 /= len(test_loader.dataset) / batch_size_test
    print('\nTest set: Hard: loss: {:.4f}, loss2: {:.4f},losst: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, test_loss2, test_loss3, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))

    return test_loss, 100. * correct / len(test_loader.dataset)


####################################################################

##############################################################

print('VGG classifier for CIFAR10 dataset:')
soft_train_losses = []
soft_train_losses1 = []
soft_train_losses2 = []
soft_train_losses3 = []
soft_train_accuracies = []
hard_train_losses = []
# hard_train_losses1= []
# hard_train_losses2 = []
hard_train_accuracies = []
soft_test_losses = []
soft_test_losses1 = []
soft_test_losses2 = []
soft_test_losses3 = []
soft_test_accuracies = []
hard_test_losses = []
# hard_test_losses1= []
# hard_test_losses2 = []
hard_test_accuracies = []

n_epochs = 350
trade_off = torch.tensor(0.8)
trade_off_t = torch.tensor(0.4)


for epoch in range(1, n_epochs + 1):

    if epoch <= 225:
        trade_off += trade_off * 0.015
        trade_off_t += trade_off_t * 0.015

        path = './experiment/1resnet_512T512.pth.tar'
        optimizer = optim.SGD(network.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-5)
    elif epoch == 226:
        trade_off += trade_off * 0.015
        trade_off_t += trade_off_t * 0.015

        path =  './experiment/1resnet_512T512_1.pth.tar'
        optimizer = optim.SGD(network.parameters(), lr=0.005, momentum=0.9, weight_decay=5e-5)
    elif epoch <= 350:

        trade_off += trade_off * 0.008
        trade_off_t += trade_off_t * 0.008
        path =  './experiment/1resnet_512T512_1.pth.tar'
        optimizer = optim.SGD(network.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-5)
    elif epoch == 351:

        path =  './experiment/2nolossv1resnet18_2D512T2d512_2.pth.tar'
        optimizer = optim.SGD(network.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-5)
    else:

        path = './experiment/2nolossv1resnet18_2D512T2d512_2.pth.tar'
        optimizer = optim.SGD(network.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-5)

    trade_off = trade_off.to(device)
    print('Trade off:{}'.format(trade_off))
    trade_off_t = trade_off_t.to(device)
    print('Trade off_t:{}'.format(trade_off_t))
    soft_train_loss, soft_train_loss1, soft_train_loss2, soft_train_loss3, soft_train_accu = train(epoch, network,optimizer, trade_off,
                                                                                                   trade_off_t)  # trade_off_p, trade_off_v, trade_off_q
    soft_train_losses.append(soft_train_loss)
    soft_train_accuracies.append(soft_train_accu)
    soft_train_losses1.append(soft_train_loss1)
    soft_train_losses2.append(soft_train_loss2)
    soft_train_losses3.append(soft_train_loss3)

    soft_test_loss, soft_test_loss1, soft_test_loss2, soft_test_loss3, soft_test_accu = soft_test(network, trade_off,
                                                                                                  trade_off_t)
    soft_test_losses.append(soft_test_loss)
    soft_test_accuracies.append(soft_test_accu)
    soft_test_losses1.append(soft_test_loss1)
    soft_test_losses2.append(soft_test_loss2)
    soft_test_losses3.append(soft_test_loss3)


    if epoch >= 300:
        hard_train_loss, hard_train_accu = hard_training_accu(network)
        hard_train_losses.append(hard_train_loss)
        hard_train_accuracies.append(hard_train_accu)

        hard_test_loss, hard_test_accu = hard_testing_accu(network)
        hard_test_losses.append(hard_test_loss)
        hard_test_accuracies.append(hard_test_accu)



    state = {
        'epoch': epoch,
        'model_state_dict': network.state_dict(),
        'trade_off': trade_off,
        'trade_off_t': trade_off_t,

        "so_Loss": soft_train_losses,
        'so_Loss1': soft_train_losses1,
        "so_Loss2": soft_train_losses2,
        'so_Accuracy': soft_train_accuracies,
        "so_test_Loss": soft_test_losses,
        'so_test_loss1': soft_test_losses1,
        'so_test_loss2': soft_test_losses2,
        "so_Loss3": soft_train_losses3,
        'so_test_loss3': soft_test_losses3,
        "so_test_Accuracy": soft_test_accuracies,

        "ha_Loss": hard_train_losses,

        'ha_Accuracy': hard_train_accuracies,
        "ha_test_Loss": hard_test_losses,

        "ha_test_Accuracy": hard_test_accuracies

    }
    torch.save(state, path)

print('Finished Training.')


path = './experiment/1resnet_512T512_1.pth.tar'

def load_checkpoint(checkpoint_path):
    checkpoint = torch.load(checkpoint_path)
    #model.load_state_dict(checkpoint['model_state_dict'])
    #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    return checkpoint['so_test_Accuracy'], checkpoint['ha_Accuracy'], checkpoint["ha_test_Accuracy"]


test_accu, ha_accu, ha_accu_test = load_checkpoint(path)

#print(test_accu)
print(np.max(test_accu))
print(ha_accu)
print(ha_accu_test)

